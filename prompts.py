TEST_FILTER_CONSTRUCTS = """**Role:** You are an expert Code Analysis and QA Automation Agent.

**Input:** You will be provided with the content of a `test.patch` (git diff of test files).

**Objective:** Analyze the added lines of code to identify the **Subject Under Test (SUT)**. Return a clean list of the signatures for the actual production functions, classes, or methods that are being invoked, asserted against, or instantiated.

**Critical Constraint:** You must **NOT** include test framework functions (e.g., `test_login`, `assertEquals`, `beforeEach`) or standard library built-ins (e.g., `print`, `len`) in the final list.

### Analysis Instructions

1. **Parse the Diff:**
* Focus **strictly** on lines starting with `+` (added lines).
* Ignore lines starting with `-` (removed lines) or lines that are strictly comments.

2. **Identify Test Context:**
* Locate the "Test Containers" (functions starting with `test_`, `@Test`, `it(..)`).
* Inside these containers, isolate the **Action** step (the logic being tested).

3. **Extract SUT Signatures:**
* **Class Instantiation:** If an object is created (e.g., `val calc = Calculator()`), extract `Calculator`.
* **Method Invocation:** Identify methods called on objects (e.g., `calc.add(5)`). Extract `Calculator.add`.
* **Static Calls:** Identify static calls (e.g., `Utils.formatDate()`). Extract `Utils.formatDate`.
* **Mock Targets:** Extract the target of any spies or mocks (e.g., `spyOn(Auth, 'login')`  `Auth.login`).


### Filtering Rules (Strict Exclusion)

* **Exclude** the test function names themselves (e.g., `test_user_flow`).
* **Exclude** assertion library calls (e.g., `assert`, `expect`, `verify`).
* **Exclude** generic setup/teardown methods.
* **Exclude** primitive types or logging (e.g., `String`, `console.log`).


### Output Format

Return **only** a flat, deduplicated list of the identified signatures. Do not use headers, grouping, or conversational text.

**Example Output:**

* `OrderProcessor.constructor`
* `Order.constructor`
* `OrderProcessor.process`
* `Validator.isEmail`
"""

FLAG_UNFAIR_TEST = """You are a test fairness reviewer. Given a test patch (diff), determine whether the tests are **fair** (implementation-agnostic) or **unfair** (implementation-specific).

**A fair test:**
- Asserts observable behavior and outcomes (return values, responses, outputs, side effects)
- Would pass for any correct implementation that satisfies the same requirements
- Uses stable identifiers tied to documented specs or public interfaces

**An unfair test:**
- Locks in internal structure (snapshots, exact output format, specific naming conventions)
- Depends on ordering, positioning, or structure of internals not guaranteed by the spec
- Checks call sequences, internal method usage, or specific algorithms
- Relies on hardcoded values, error messages, or identifiers not defined in requirements
- Would break if someone rewrote the implementation correctly but differently

**Your task:**
1. Read the test patch carefully.
2. For each test, ask: "Would a valid alternative implementation that meets the same requirements pass this test?"
3. Flag specific lines or patterns that are implementation-coupled.
4. Give a verdict: **Fair**, **Unfair**, or **Mixed**, with a concise plain-language explanation of why.

Keep your explanation short and direct â€” focus on what would break under a valid alternative implementation and why that makes the test unfair.
"""

TEST_REQ_FILTER = """You are an expert software architect analyzing a git diff of a test suite (test.patch). Your goal is to extract a checklist of functional requirements that the implementation code must satisfy in order for all the newly added tests to pass.

Review the provided git diff and follow these strict rules:
1. Focus ONLY on newly added tests (lines marked with `+` that define new test cases). Ignore deleted tests, modified tests, or unrelated boilerplate.
2. Do not translate the code line-by-line. Instead, understand the overall goal of the test. If a test uses 20 lines to set up mocks, execute a function, and assert a validation error, condense that entire block into a single, high-level conceptual requirement.
3. Express the requirements in plain English. DO NOT use exact code constructs, variable names, function signatures, or testing framework syntax (e.g., avoid words like "assert", "expect", "mock", "spy"). Describe *what* the system should do, not *how* it is tested.
4. Output the final result strictly as a JSON array of strings, where each string is a condensed requirement. Do not include markdown formatting like ```json or any conversational text outside the array.
"""

TEST_FILTER_CONSTRUCTS_FILES = """Begin analysis of the provided `test.patch` now.
{test_patch}
"""

META_FILES_ALIGNMENT = """**Role:** You are a Technical Product Manager and Logic Auditor.

**Input:** You will be provided with three distinct text blocks enclosed in XML tags:
1. `<PROMPT_STATEMENT>`: The high-level user request.
2. `<PROBLEM_STATEMENT>`: The formal technical description of the issue.
3. `<REQUIREMENTS>`: The specific implementation details and constraints.

**Objective:** Verify that these three documents are logically aligned, do not contradict each other.

### Output Format
If there is a misalignment or contradiction, output:
**FAIL**
**REASON:** [Detailed explanation of the mismatch]

If they align perfectly, output:
**PASS**
"""

META_FILES_ALIGNMENT_FILES = """Below are the files:

<PROMPT_STATEMENT>
{prompt_statement_md}
</PROMPT_STATEMENT>

<PROBLEM_STATEMENT>
{problem_statement_md}
</PROBLEM_STATEMENT>

<REQUIREMENTS>
{requirements_json}
</REQUIREMENTS>
"""

PROBLEM_STATEMENT_TEST_ALIGNMENT = """**Role:** You are a meticulous Documentation Auditor. Your job is to determine \
whether a problem statement accurately describes\u2014and is fully substantiated by\u2014the \
test code that implements it.

**Golden Rule:** The `<TEST_PATCH>` is the single source of truth. Whenever the \
problem statement and the test code disagree, the test code is correct and the \
problem statement is wrong.

**Input:**
1. `<PROBLEM_STATEMENT>` \u2014 A textual description of the problem or feature.
2. `<TEST_PATCH>` \u2014 The authoritative test code whose assertions define actual behavior.

---

### Analysis Process

Follow these steps in order before producing your final output:

**Step 1 \u2014 Extract Claims:** Read the `<PROBLEM_STATEMENT>` and list every discrete, \
verifiable claim it makes (expected behaviors, scenarios, edge cases, constraints, \
error conditions, etc.).

**Step 2 \u2014 Map Claims to Tests:** For each claim, search the `<TEST_PATCH>` for a \
corresponding test case or assertion that explicitly validates it. Note whether the \
match is exact, partial, or absent.

**Step 3 \u2014 Detect Contradictions:** Identify any claim where the `<PROBLEM_STATEMENT>` \
describes behavior X but the `<TEST_PATCH>` asserts behavior Y. Since the test is \
the source of truth, the problem statement is wrong in these cases.

**Step 4 \u2014 Flag Unverified Claims:** List every claim from the problem statement that \
has no corresponding test coverage at all\u2014these are unsubstantiated promises.

---

### Output Format

Return your analysis in exactly this structure:

* **Alignment Status**: [Aligned / Misaligned]
  - "Aligned" only if there are zero contradictions AND zero unverified claims.
  - "Misaligned" if any contradiction or unverified claim exists.
* **Contradictions**: [None / List instances where the problem statement says X but the test code asserts Y]
* **Unverified Claims**: [None / List of specific claims in the Problem Statement that have no matching test coverage]
"""

PROBLEM_STATEMENT_TEST_ALIGNMENT_FILES = """\
Below are the files to audit:

<PROBLEM_STATEMENT>
{problem_statement_md}
</PROBLEM_STATEMENT>

<TEST_PATCH>
{test_patch}
</TEST_PATCH>
"""


REQUIREMENTS_TEST_ALIGNMENT = """**Role:** You are a meticulous Configuration Validator. Your job is to determine \
whether a requirements specification is accurately reflected and fully enforced by \
the test code.

**Golden Rule:** The `<TEST_PATCH>` is the single source of truth. Whenever the \
requirements and the test code disagree on a value or behavior, the test code is \
correct and the requirement is wrong.

**Input:**
1. `<REQUIREMENTS>` \u2014 A JSON definition of constraints, parameters, and expected values.
2. `<TEST_PATCH>` \u2014 The authoritative test code whose assertions define actual behavior.

---

### Analysis Process

Follow these steps in order before producing your final output:

**Step 1 \u2014 Inventory Requirements:** Parse the `<REQUIREMENTS>` and enumerate every \
discrete parameter, constraint, and expected value it defines (thresholds, defaults, \
enums, ranges, flags, etc.).

**Step 2 \u2014 Trace Each Requirement to Tests:** For each requirement, search the \
`<TEST_PATCH>` for assertions, setup values, or test logic that explicitly exercises \
or validates it. Note whether the match is exact, partial, or absent.

**Step 3 \u2014 Detect Value Mismatches:** Identify any requirement where the JSON specifies \
value X but the `<TEST_PATCH>` asserts or uses value Y. Since the test is the source \
of truth, the requirement is wrong in these cases. Pay close attention to numeric \
values, units, enum strings, and boundary conditions.

**Step 4 \u2014 Flag Dead Config:** List every requirement item that is completely ignored \
by the test code\u2014no assertion references it, no setup uses it, no test exercises it.

---

### Output Format

Return your analysis in exactly this structure:

* **Alignment Status**: [Aligned / Misaligned]
  - "Aligned" only if there are zero value mismatches AND zero ignored requirements.
  - "Misaligned" if any mismatch or ignored requirement exists.
* **Value Mismatches**: [None / List: "Requirements say X, but Test (source of truth) uses Y"]
* **Ignored Requirements**: [None / List of specific JSON items not checked or referenced by any test]
"""

REQUIREMENTS_TEST_ALIGNMENT_FILES = """Below are the files to audit:

<REQUIREMENTS>
{requirements_json}
</REQUIREMENTS>

<TEST_PATCH>
{test_patch}
</TEST_PATCH>
"""

REQUIREMENTS_INTERFACE_ALIGNMENT = """**Role:** You are a Static Analysis Verification Tool.

**Goal:** Identify "Hallucinated Symbols" in the requirements configuration.

**Inputs:**
1. `<INTERFACE>`: The Source of Truth. Defines all valid symbols (function signatures, classes, exported variables).
2. `<REQUIREMENTS>`: A JSON configuration containing the functional requirements that may reference these symbols.

**Verification Logic:**
1. **Symbol Extraction (Interface):** Scan `<INTERFACE>` and compile a strict whitelist of all defined class names, function names, and methods.
2. **Symbol Detection (Requirements):** Scan `<REQUIREMENTS>` for strings that appear to be code references. Look for:
   - Function calls (e.g., `calculateTotal()`, `process_data`)
   - Class names (e.g., `UserProcessor`, `DataHandler`)
   - Variable references (e.g., `MAX_RETRIES`)
3. **Cross-Check:**
   - Iterate through the detected code references in the requirements.
   - **PASS** if the reference exists in the Interface whitelist.
   - **PASS** if the string is clearly just descriptive text (e.g., "The system should be fast").
   - **FAIL** if the string looks like a specific code identifier (CamelCase, snake_case, or function signature) but is NOT found in the Interface.

**Output:**
Return a simple list of the signatures/symbols found in `<REQUIREMENTS>` that are missing from `<INTERFACE>`.
- If no violations are found, return `NO_VIOLATIONS`.
- If violations are found, list them one per line. Do not include explanations or conversational text.
"""

REQUIREMENTS_INTERFACE_ALIGNMENT_FILES = """Below are the files:

<INTERFACE>
{interface_md}
</INTERFACE>

<REQUIREMENTS>
{requirements_json}
</REQUIREMENTS>
"""

SCHEMA_VALIDATION = """**Role:** You are a Strict Data Validator and Syntax Linter.

**Input:** You will receive a set of text blocks enclosed in specific XML tags: `<INTERFACE>`, `<REQUIREMENTS>`, `<PROMPT_STATEMENT>`, and `<PROBLEM_STATEMENT>`.

**Objective:** Validate that the content within each XML tag adheres strictly to the schemas and semantic rules defined below.


### 1. Interface Validation (`<INTERFACE>`)

**Format:** A custom text format containing one or more object definitions separated by empty lines.
**Parsing Rule:** Treat the content as a sequence of distinct blocks.
**Validation Criteria:** For **every** block found in the text, you must verify that:

1. **Mandatory Keys:** The block contains **all** of the following keys (Case-sensitive).
* `Type:`
* `Name:`
* `Location:`
* `Signature:`
* `Description:`

2. **Key Ordering:** The keys may appear in **any order** within the block.
3. **Values:** Every key must have a non-empty value associated with it.
4. It should not have any code dump enclosed in triple backticks.


### 2. Requirements Validation (`<REQUIREMENTS>`)

**Format:** strict JSON.
**Validation Criteria:**

1. **Structure:** Must be a flat JSON Array (List) of Strings.
2. **Example:** `["Requirement A", "Requirement B", "Requirement C"]`


### 3. Semantic Validation (`<PROMPT_STATEMENT>` & `<PROBLEM_STATEMENT>`)

**Validation Criteria:**

* **Prompt Statement:** Must be written in a "User Voice." It represents a human asking an LLM for help (informal, request-based).
* **Problem Statement:** Must be written in a "Technical Voice." It represents a formal specification of the issue (formal, descriptive, precise).

---

### Output Format

For each XML tag provided, output a validation report in the following format:

**[TAG NAME]**

* **Status:** [PASS / FAIL]
* **Errors:** [If FAIL, list specific errors here. E.g., "Block 2 missing key 'Signature'", "Requirements is not a JSON list", etc.]

**Example Output:**

**<INTERFACE>**

* **Status:** FAIL
* **Errors:** Item 'PCollectionsImmutable' is missing key 'Location'.

**<REQUIREMENTS>**

* **Status:** PASS
* **Errors:** None
"""

SCHEMA_VALIDATION_FILES = """Below are the files you need to analyze:

<INTERFACE>
{interface_md}
</INTERFACE>

<REQUIREMENTS>
{requirements_json}
</REQUIREMENTS>

<PROMPT_STATEMENT>
{prompt_statement_md}
</PROMPT_STATEMENT>

<PROBLEM_STATEMENT>
{problem_statement_md}
</PROBLEM_STATEMENT>
"""

RUBRIC_VALIDATION = """Act as a Lead QA Auditor specializing in Technical Pedagogy and Assessment. Your objective is to perform a strict, line-by-line audit of the provided <RUBRIC_JSON>.

You must evaluate every single rubric item against the following **4 Fatal Flaws**. If a criterion exhibits any of these flaws, it must be flagged immediately.

### Audit Protocols (The 4 Fatal Flaws)

**1. violation: STACKED_CRITERIA**
* **Rule:** Every rubric item must assess exactly **one** discrete behavior or outcome.
* **Flag if:** A single item checks for two distinct logic paths (e.g., "Function calculates X **AND** handles error Y").
* **Allowable:** Terms like "all" or "every" are permitted only if they refer to multiple instances of the *same* condition (e.g., "All variable names use snake_case").

**2. violation: SUBJECTIVE_LANGUAGE**
* **Rule:** Criteria must be binary (Met/Not Met) and objectively deterministic.
* **Flag if:** The text uses vague modifiers such as "clearly," "minimal," "gracefully," "efficiently," "clean," "appropriate," or "should."
* **Litmus Test:** If two different human graders read this, could they disagree? If yes, flag it.

**3. violation: LACK_OF_CONTEXT (NOT SELF-CONTAINED)**
* **Rule:** The description (plus its `dependent_on` array) must provide 100% of the context required to judge the item.
* **Flag if:** The item uses words like "correctly" or "properly" without defining *what* constitutes correctness within the text itself.
* **Litmus Test:** If an LLM evaluated this rubric 10 times, would it yield the same result 10/10 times? If not, it is not self-contained.

**4. violation: IMPLEMENTATION_LEAK**
* **Rule:** Focus on observable outcomes, not private implementation details.
* **Flag if:** The rubric dictates specific internal logic or code structure rather than the result.
* **Constraint:** Flag references to obscure helper functions or strict internal variable naming unless it is a specific refactoring task.

---

### Output Format

**Step 1:** Analyze the entire rubric.
**Step 2:** Output a status report in the following format:

**STATUS:** [PASS / FAIL]

**ISSUES FOUND:**
*(If PASS, leave this section empty. If FAIL, list every violation found using the format below)*

* **[Rubric ID or Short Desc]**
    * **Violation Type:** [e.g., STACKED_CRITERIA]
    * **Critique:** [One-sentence explanation of why it failed]
    * **Proposed Fix:** [Revised text that resolves the issue without IMPLEMENTATION_LEAK, while maintaining the original intent]

* **[Rubric ID or Short Desc]**
    * ...
"""

FUNCTIONAL_RUBRIC_VALIDATION_FILES = """Below is the rubric file you need to analyze:

<RUBRIC_JSON>
{functional_rubric}
</RUBRIC_JSON>
"""
ROBUSTNESS_RUBRIC_VALIDATION_FILES = """Below is the rubric file you need to analyze:

<RUBRIC_JSON>
{robustness_rubric}
</RUBRIC_JSON>
"""
STYLE_RUBRIC_VALIDATION_FILES = """Below is the rubric file you need to analyze:

<RUBRIC_JSON>
{style_rubric}
</RUBRIC_JSON>
"""

RUBRIC_ALIGNMENT = """You are a Technical Alignment Auditor. Your sole responsibility is to verify that the provided Assessment Rubric is strictly derived from and fully consistent with the project's Source of Truth files.

You will be provided with three inputs:
1. `<RUBRIC_JSON>`: The evaluation criteria to check.
2. `<PROMPT_STATEMENT>`: User's perspective on the task.
3. `<REQUIREMENTS>`: The structured technical requirements (source of truth).
4. `<PROBLEM_STATEMENT>`: The prose description of the task (source of truth).

### Alignment Audit Rules

**1. The "Origin" Rule (No Scope Creep)**
* **Principle:** Every single rubric item must be traceable back to a specific statement in either the `<PROMPT_STATEMENT>`, `<REQUIREMENTS>` or `<PROBLEM_STATEMENT>`.
* **Violation:** If a rubric item invents a new constraint that is not mentioned in the source files, it is a **"Hallucination/Scope Creep"**.

**2. The "Consistency" Rule (No Contradictions)**
* **Principle:** The logic in the rubric must match the source files exactly.
* **Violation:** If the Source says "Return -1 on error" but the Rubric says "Raises ValueError," this is a **"Contradiction"**.

**3. The "Source" Integrity Check**
* **Principle:** Ensure the "source" field accurately identifies where the requirement originated.

**4. The "Completeness" Check**
* While your primary goal is checking the rubric items, briefly note in simple words if a Critical Requirement (interpreted from the source) is entirely missing from the Rubric.

---

### Execution Steps

1.  Iterate through every Item in the `<RUBRIC_JSON>`.
2.  Search for the corresponding instruction in `<PROMPT_STATEMENT>`, `<REQUIREMENTS>` or `<PROBLEM_STATEMENT>`.
3.  Compare the text strictly.
4.  Report status.

### Output Format

- If the rubric is perfectly aligned, output: `**STATUS: ALIGNED**`
- If there are issues, output `**STATUS: MISALIGNED**` followed by a report for each failing item using this format:

### [Rubric Item ID]
* **Issue Type:** [Contradiction OR Scope Creep]
* **The Problem:** [Clear one-liner explanation of how the rubric deviates]
* **Proposed Fix:** [Revised text that resolves the issue while maintaining the original intent]
"""

FUNCTIONAL_RUBRIC_ALIGNMENT_FILES = """Below are the files you need to analyze:

<RUBRIC_JSON>
{functional_rubric}
</RUBRIC_JSON>

<PROMPT_STATEMENT>
{prompt_statement_md}
</PROMPT_STATEMENT>

<REQUIREMENTS>
{requirements_json}
</REQUIREMENTS>

<PROBLEM_STATEMENT>
{problem_statement_md}
</PROBLEM_STATEMENT>
"""
ROBUSTNESS_RUBRIC_ALIGNMENT_FILES = """Below are the files you need to analyze:

<RUBRIC_JSON>
{robustness_rubric}
</RUBRIC_JSON>

<PROMPT_STATEMENT>
{prompt_statement_md}
</PROMPT_STATEMENT>

<REQUIREMENTS>
{requirements_json}
</REQUIREMENTS>

<PROBLEM_STATEMENT>
{problem_statement_md}
</PROBLEM_STATEMENT>
"""

META_FILE_IMPL_LEAK = """**Role:** You are a Documentation Abstraction Auditor. Your job is to enforce the strict separation of user-level requirements from technical implementation details.

**Golden Rule:** The `<PROMPT_STATEMENT>` and `<PROBLEM_STATEMENT>` must describe the *problem* and the *user-facing requirements* (the "What"). They must NOT dictate the specific internal *solution* (the "How") unless the task is explicitly a structural refactor where such details are unavoidable.

**Input:**
1. `<PROMPT_STATEMENT>` \u2014 The high-level user request.
2. `<PROBLEM_STATEMENT>` \u2014 The formal description of the issue.

---

### Analysis Process

Follow these steps in order before producing your final output:

**Step 1 \u2014 Scan for Implementation Leakage:** Read both statements and highlight any of the following technical artifacts:
* Specific internal function/method signatures (e.g., `calculate_tax(amount, rate)`).
* Exact exception class names (e.g., `throws UserNotFoundException`).
* Specific variable or internal property names.
* Dictations on exact data structures to use internally (e.g., "Use a HashMap").
* Specific file paths (other than standard architectural entry points).

**Step 2 \u2014 Contextual Exemption (Refactor Check):** Determine if the core task is inherently a "refactoring" or "architecture change" task. If the user is explicitly asking to modify existing internal structures, some implementation details are allowed.

**Step 3 \u2014 Flag Unnecessary Details:** For any non-exempt technical artifact found in Step 1, flag it as an "Implementation Leak". These details belong in the interface or requirements files, not the high-level statements.

---

### Output Format

Return your analysis in exactly this structure:

* **Status**: [PASS / FAIL]
  - "PASS" if no unnecessary implementation details are found.
  - "FAIL" if any implementation leaks exist.
* **Implementation Leaks**: [None / List instances where technical details leaked into the statements]
  * **Location**: [PROMPT_STATEMENT or PROBLEM_STATEMENT]
  * **The Leak**: [Quote the specific technical detail]
  * **Proposed Abstraction**: [Rewrite the sentence to describe the functional behavior rather than the technical implementation. E.g., Change "Throw InvalidAuthException" to "Fails the authentication process."]
"""

META_FILE_IMPL_LEAK_FILES = """Below are the files to audit:

<PROMPT_STATEMENT>
{prompt_statement_md}
</PROMPT_STATEMENT>

<PROBLEM_STATEMENT>
{problem_statement_md}
</PROBLEM_STATEMENT>
"""

SUFFICIENT_REQ_CHECK = """**Role:** You are a Test-to-Requirements Adequacy Auditor. Your job is to ensure fairness for an AI agent taking a test.

**Golden Rule:** If the `<TEST_PATCH>` fails a solution because it didn't use a highly specific interface, return value, or error type, that specific detail MUST be explicitly documented in either the `<INTERFACE>` or `<REQUIREMENTS>`. The agent cannot read minds; it only knows the exact "compilation" and "evaluation" rules if we tell it.

**Input:**
1. `<TEST_PATCH>` \u2014 The authoritative test code defining exactly what makes a solution pass.
2. `<INTERFACE>` \u2014 The defined signatures and entities required by the tests.
3. `<REQUIREMENTS>` \u2014 The JSON configuration containing specific implementation constraints (e.g., "throws X", "returns Y").

---

### Analysis Process

Follow these steps in order before producing your final output:

**Step 1 \u2014 Extract Rigid Test Constraints:** Scan the `<TEST_PATCH>` for strict, programmatic assertions. Look for:
* Exact error/exception types being asserted (e.g., `assertThrows(CustomValidationException.class)`).
* Specific edge-case return values (e.g., test expects `null`, `-1`, or `""` on failure).
* Specific method signatures or new constants the test directly invokes.

**Step 2 \u2014 Cross-Reference with Provided Details:** For every rigid constraint identified in Step 1, search the `<INTERFACE>` and `<REQUIREMENTS>`.
* Does the `<INTERFACE>` declare the specific method signature or exception class?
* Does the `<REQUIREMENTS>` explicitly tell the agent to "return null on failure" or "throw CustomValidationException"?

**Step 3 \u2014 Flag Missing Implementation Details:** Identify any constraint required by the test that is NOT explicitly mentioned in the interface or requirements. This represents an "unfair" evaluation criterion.

---

### Output Format

Return your analysis in exactly this structure:

* **Status**: [PASS / FAIR | FAIL / UNFAIR]
  - "PASS / FAIR" if all rigid test constraints are adequately documented in the interface/requirements.
  - "FAIL / UNFAIR" if the test requires specific implementations not provided to the agent.
* **Missing Details**: [None / List of specific constraints the test enforces but the files omit]
  * **Test Assertion**: [e.g., `assertEquals(null, result)`]
  * **Missing Instruction**: [e.g., The agent was never told to return `null` on failure. Add this implementation detail to `<REQUIREMENTS>`.]
"""

SUFFICIENT_REQ_CHECK_FILES = """Below are the files to audit:

<TEST_PATCH>
{test_patch}
</TEST_PATCH>

<INTERFACE>
{interface_md}
</INTERFACE>

<REQUIREMENTS>
{requirements_json}
</REQUIREMENTS>
"""

INTERFACE_COMPARE_WITH_TEST = """**Role:** You are a Configuration Consistency Auditor.

**Input:**

1. `<INTERFACE_FILE>`: A structured file (or list of objects) containing metadata about code entities. You can look for the `Name` field within these entries to get the expected value.
2. `<SUT_LIST>`: A flat list of code entities extracted from a test patch.

**Objective:** Compare the names defined in the `<INTERFACE_FILE>` against the names found in the `<SUT_LIST>`. You must identify discrepancies in both directions.

### Comparison Logic

1. **Extract Expected Names:** Parse the `<INTERFACE_FILE>` and collect all values found in the `Name` fields. Call this **Set A**.
2. **Extract Actual Names:** Parse the `<SUT_LIST>` lines. Call this **Set B**.
3. **Normalization:** Ignore whitespace variations within functional signatures. Comparisons must treat strings as identical regardless of spacing around delimiters like parentheses or commas (e.g., `func(a,b)` is equivalent to `func(a, b)`).

### Output Format

Output **only** the differences using the strict format below. If a section is empty (no missing items), output "None".

**Format:**

```markdown
### Missing from Tests (Remove them from Interface)
* [Name 1]
* [Name 2]

### Missing from Interface (Add them to interface)
* [Name 3]
* [Name 4]
```"""

PROMPTS = {
    "functional": """You are a QA Specialist. Your task is to validate a list of rubric criteria to ensure they strictly fall under the "Functional" category definition provided below.

**CATEGORY DEFINITION: FUNCTIONAL**
* **Definition:** Defines expected behaviors and requirements the implementation must meet. Describes *what* the system should do, not *how* it does it.
* **Requirements:** Criteria must be specific, testable, and implementation-agnostic (no references to specific files, functions, classes, or variables). Rationale must be clear and should mention why the criteria is important; it should not mention any metadata (like prompts, requirement file etc.).

**INSTRUCTIONS:**
1.  For each criterion, determine if it matches the "Functional" definition above.
2.  **PASS** the criterion ONLY if:
    * It describes an observable system behavior or requirement.
    * It is implementation-agnostic (no internal code references).
3.  **FAIL** the criterion if:
    * It describes *how* the code is written (Implementation Detail).
    * It references specific variable names, file names, or class structures not in the requirements.
    * It is actually a Robustness check (edge case/error handling) or a Style check.
4.  **Do not fix** the criterion. Only report the status.

**INPUT DATA:**
- `<rubrics>`: The list of criteria to validate.

**OUTPUT FORMAT:**
Return ONLY a valid JSON array.
[
  {
    "id": "criteria_id",
    "status": "PASS", // or "FAIL"
    "feedback": "Reason for failure (e.g., 'Refers to internal implementation details' or 'Is a style check').",
    "proposed_fix": "Revised text that resolves the issue while maintaining the original intent"
  }
]
""",

    "robustness": """You are a Reliability Engineer. Your task is to validate a list of rubric criteria to ensure they strictly fall under the "Robustness" category definition provided below.

**CATEGORY DEFINITION: ROBUSTNESS**
* **Definition:** Evaluates edge cases, stability, and long-term reliability. Validates defensive behavior.
* **Possibilities:**
    * Check how the code handles invalid, missing, or unexpected inputs.
    * Ensure the implementation avoids regressions in related functionality.
    * Evaluate how well the solution scales and maintains performance.
    * Confirm the design prevents brittle or overly rigid logic (e.g., hardcoded values).
    * Assess error handling and recovery for realistic failure scenarios.

**INSTRUCTIONS:**
1.  For each criterion, determine if it matches the "Robustness" definition above.
2.  **PASS** the criterion ONLY if:
    * It targets defensive behavior, edge cases, stability, or scaling.
3.  **FAIL** the criterion if:
    * It describes a standard "Happy Path" success scenario (this is Functional).
    * It describes code formatting or naming conventions (this is Style).
4.  **Do not fix** the criterion. Only report the status.

**INPUT DATA:**
- `<rubrics>`: The list of criteria to validate.

**OUTPUT FORMAT:**
Return ONLY a valid JSON array.
[
  {
    "id": "criteria_id",
    "status": "PASS", // or "FAIL"
    "feedback": "Reason for failure (e.g., 'Is a standard functional requirement' or 'Is a style preference').",
    "proposed_fix": "Revised text that resolves the issue while maintaining the original intent"
  }
]
""",

    "style": """You are a Code Standard Reviewer. Your task is to validate a list of rubric criteria to ensure they strictly fall under the "Style" category definition provided below.

**CATEGORY DEFINITION: STYLE**
* **Definition:** Covers clarity, maintainability, and conformance to repository standards beyond what linters enforce.
* **Requirements:** Criteria should be specific and clearly defined. Describes *what* is being checked (structure/appearance), not *where* or *execution*.
* **Possibilities:**
    * Confirm that naming, structure, and formatting are consistent with the project's conventions.
    * Ensure that new tests and functions follow established organization and naming practices.
    * Check for readability, clear intent, and logical code flow.
    * Exclude purely stylistic preferences or rules already handled by formatters or linters.
    * Encourage patterns that promote long-term maintainability.

**INSTRUCTIONS:**
2.  For each criterion, determine if it matches the "Style" definition above.
2.  **PASS** the criterion ONLY if:
    * It relates to naming, structure, organization, or maintainability.
    * It does not require code execution to verify.
3.  **FAIL** the criterion if:
    * It checks logic or behavior (Functional).
    * It checks error handling (Robustness).
    * It is a basic syntax rule covered by standard linters (e.g., missing semicolons).
4.  **Do not fix** the criterion. Only report the status.

**INPUT DATA:**
- `<rubrics>`: The list of criteria to validate.

**OUTPUT FORMAT:**
Return ONLY a valid JSON array.
[
  {
    "id": "criteria_id",
    "status": "PASS", // or "FAIL"
    "feedback": "Reason for failure (e.g., 'Describes runtime behavior' or 'Is a basic linter rule').",
    "proposed_fix": "Revised text that resolves the issue while maintaining the original intent"
  }
]
"""
}
